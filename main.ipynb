{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tweet_prepossesssing_and_clustering.ipynb\n",
      "importing Jupyter notebook from chainning_and_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from tweet_prepossesssing_and_clustering import is_english,extract_hashtag,remove_stop_words,lemmotize,tokenize,generate_bow,count_dict,tf_idf_vector,one_hot_encoding_vector,cosine_similarity,jaccard_similarity,dice_similarity,k_means_clustering,most_frequent_words,generate_word_cloud,iterative_clustering\n",
    "from chainning_and_analysis import calculate_silhouette_scores,calculate_cluster_weights,event_clusters_filter,generate_clusters,write_clusters_to_text_and_hashtags,read_clusters_from_file,read_hashtags_from_file,centroid,calculate_similarity,textual_similarity,create_bipartite_graph,apply_hungarian_method,create_cluster_chains,get_folder_names,write_cluster_chains,analyze_sentiment,plot_pie_chart,generate_emoji_image,generate_word_cloud,analyze_and_visualize_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19811b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import emoji\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import emojis\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c66b62d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_path = os.getcwd()\n",
    "folder_path = os.path.join(folder_path,'output_tweets')\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path, names=['Text'])\n",
    "        df = df[df['Text'].apply(is_english)]\n",
    "        extract_hashtag(df)\n",
    "        tokenize(df)\n",
    "        df['filtered_tokens'] = df['Tokenized_Tweets'].apply(remove_stop_words)\n",
    "        df = df[df['filtered_tokens'].apply(lambda x: len(x) > 0)]\n",
    "        df.reset_index(drop = True,inplace = True)\n",
    "        bag_of_words = generate_bow(df['filtered_tokens'])\n",
    "        vector = tf_idf_vector(df['filtered_tokens'].tolist(),bag_of_words.keys())\n",
    "        vector = np.array(vector)\n",
    "        cluster,no_of_clusters = iterative_clustering(vector,0.1)\n",
    "        \n",
    "        cluster_qualities = calculate_silhouette_scores(np.array(vector),np.array(cluster))\n",
    "        cluster_weights = calculate_cluster_weights(vector, cluster)\n",
    "        event_clusters = event_clusters_filter(cluster_qualities,cluster_weights,0.7,6.0)\n",
    "        event_clusters_tweets,event_clusters_hashtags = generate_clusters(list(df['filtered_tokens']),cluster,event_clusters,list(df['Hashtag']))\n",
    "        no_of_clusters = len(event_clusters)\n",
    "        if no_of_clusters < 1:\n",
    "            continue\n",
    "        word_sets = most_frequent_words(event_clusters_tweets,no_of_clusters)\n",
    "        folder_name = filename[:len(filename) - 4]\n",
    "        folder_name = os.path.join('event_tweets',folder_name)\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "        file_path = os.getcwd()\n",
    "        file_path = os.path.join(file_path,folder_name)\n",
    "        event_cluster_file = os.path.join(file_path,'event_clusters.txt')\n",
    "        event_hashtags_file = os.path.join(file_path,'event_hashtags.txt')\n",
    "        write_clusters_to_text_and_hashtags(event_clusters_tweets,event_clusters_hashtags,event_cluster_file,event_hashtags_file)\n",
    "        for i in range(no_of_clusters):\n",
    "            print(sorted(word_sets[i].items(),key = operator.itemgetter(1),reverse = True)[:10])\n",
    "            generate_word_cloud(word_sets[i], i, os.path.join(file_path,f\"img{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fbcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(),'event_tweets')\n",
    "timestamps = get_folder_names(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86451781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-23_08-00_2022-10-23_08-05 and 2022-10-23_08-05_2022-10-23_08-10\n",
      "2022-10-23_08-05_2022-10-23_08-10 and 2022-10-23_08-10_2022-10-23_08-15\n",
      "2022-10-23_08-10_2022-10-23_08-15 and 2022-10-23_08-20_2022-10-23_08-25\n",
      "2022-10-23_08-20_2022-10-23_08-25 and 2022-10-23_08-25_2022-10-23_08-30\n",
      "2022-10-23_08-25_2022-10-23_08-30 and 2022-10-23_08-30_2022-10-23_08-35\n",
      "2022-10-23_08-30_2022-10-23_08-35 and 2022-10-23_08-35_2022-10-23_08-40\n",
      "2022-10-23_08-35_2022-10-23_08-40 and 2022-10-23_08-40_2022-10-23_08-45\n",
      "2022-10-23_08-40_2022-10-23_08-45 and 2022-10-23_08-45_2022-10-23_08-50\n",
      "2022-10-23_08-45_2022-10-23_08-50 and 2022-10-23_08-50_2022-10-23_08-55\n",
      "2022-10-23_08-50_2022-10-23_08-55 and 2022-10-23_08-55_2022-10-23_09-00\n",
      "2022-10-23_08-55_2022-10-23_09-00 and 2022-10-23_09-00_2022-10-23_09-05\n",
      "2022-10-23_09-00_2022-10-23_09-05 and 2022-10-23_09-05_2022-10-23_09-10\n",
      "2022-10-23_09-05_2022-10-23_09-10 and 2022-10-23_09-10_2022-10-23_09-15\n",
      "2022-10-23_09-10_2022-10-23_09-15 and 2022-10-23_09-15_2022-10-23_09-20\n",
      "2022-10-23_09-15_2022-10-23_09-20 and 2022-10-23_09-20_2022-10-23_09-25\n",
      "2022-10-23_09-20_2022-10-23_09-25 and 2022-10-23_09-25_2022-10-23_09-30\n",
      "2022-10-23_09-25_2022-10-23_09-30 and 2022-10-23_09-30_2022-10-23_09-35\n",
      "2022-10-23_09-30_2022-10-23_09-35 and 2022-10-23_09-35_2022-10-23_09-40\n",
      "2022-10-23_09-35_2022-10-23_09-40 and 2022-10-23_09-40_2022-10-23_09-45\n",
      "2022-10-23_09-40_2022-10-23_09-45 and 2022-10-23_09-45_2022-10-23_09-50\n",
      "2022-10-23_09-45_2022-10-23_09-50 and 2022-10-23_09-50_2022-10-23_09-55\n",
      "2022-10-23_09-50_2022-10-23_09-55 and 2022-10-23_09-55_2022-10-23_10-00\n",
      "2022-10-23_09-55_2022-10-23_10-00 and 2022-10-23_10-00_2022-10-23_10-05\n",
      "2022-10-23_10-00_2022-10-23_10-05 and 2022-10-23_10-05_2022-10-23_10-10\n",
      "2022-10-23_10-05_2022-10-23_10-10 and 2022-10-23_10-10_2022-10-23_10-15\n",
      "2022-10-23_10-10_2022-10-23_10-15 and 2022-10-23_10-15_2022-10-23_10-20\n",
      "2022-10-23_10-15_2022-10-23_10-20 and 2022-10-23_10-20_2022-10-23_10-25\n",
      "2022-10-23_10-20_2022-10-23_10-25 and 2022-10-23_10-25_2022-10-23_10-30\n",
      "2022-10-23_10-25_2022-10-23_10-30 and 2022-10-23_10-30_2022-10-23_10-35\n",
      "2022-10-23_10-30_2022-10-23_10-35 and 2022-10-23_10-35_2022-10-23_10-40\n",
      "2022-10-23_10-35_2022-10-23_10-40 and 2022-10-23_10-40_2022-10-23_10-45\n",
      "2022-10-23_10-40_2022-10-23_10-45 and 2022-10-23_10-45_2022-10-23_10-50\n",
      "2022-10-23_10-45_2022-10-23_10-50 and 2022-10-23_10-50_2022-10-23_10-55\n",
      "2022-10-23_10-50_2022-10-23_10-55 and 2022-10-23_10-55_2022-10-23_11-00\n",
      "2022-10-23_10-55_2022-10-23_11-00 and 2022-10-23_11-00_2022-10-23_11-05\n",
      "2022-10-23_11-00_2022-10-23_11-05 and 2022-10-23_11-05_2022-10-23_11-10\n",
      "2022-10-23_11-05_2022-10-23_11-10 and 2022-10-23_11-10_2022-10-23_11-15\n",
      "2022-10-23_11-10_2022-10-23_11-15 and 2022-10-23_11-15_2022-10-23_11-20\n",
      "2022-10-23_11-15_2022-10-23_11-20 and 2022-10-23_11-20_2022-10-23_11-25\n",
      "2022-10-23_11-20_2022-10-23_11-25 and 2022-10-23_11-25_2022-10-23_11-30\n",
      "2022-10-23_11-25_2022-10-23_11-30 and 2022-10-23_11-30_2022-10-23_11-35\n",
      "2022-10-23_11-30_2022-10-23_11-35 and 2022-10-23_11-35_2022-10-23_11-40\n",
      "2022-10-23_11-35_2022-10-23_11-40 and 2022-10-23_11-40_2022-10-23_11-45\n",
      "2022-10-23_11-40_2022-10-23_11-45 and 2022-10-23_11-45_2022-10-23_11-50\n",
      "2022-10-23_11-45_2022-10-23_11-50 and 2022-10-23_11-50_2022-10-23_11-55\n",
      "2022-10-23_11-50_2022-10-23_11-55 and 2022-10-23_11-55_2022-10-23_12-00\n"
     ]
    }
   ],
   "source": [
    "resulting_chains = create_cluster_chains(timestamps,folder_path,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5675aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_dict(folder_path):\n",
    "    clusters_dict = {}\n",
    "    folders = sorted(os.listdir(folder_path))\n",
    "    for i in range(len(folders)-1):\n",
    "        current_folder = folders[i]\n",
    "        next_folder = folders[i+1]\n",
    "\n",
    "        current_file_path = os.path.join(folder_path, current_folder, 'event_clusters.txt')\n",
    "        next_file_path = os.path.join(folder_path, next_folder, 'event_clusters.txt')\n",
    "\n",
    "        current_clusters = read_clusters_from_file(current_file_path)\n",
    "        next_clusters = read_clusters_from_file(next_file_path)\n",
    "\n",
    "        clusters_dict[current_folder] = current_clusters\n",
    "        clusters_dict[next_folder] = next_clusters\n",
    "\n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3ff977",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = create_clusters_dict(os.path.join(os.getcwd(),'event_tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614cd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cluster_chains(resulting_chains,os.getcwd(),clusters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb638395",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_chains_folder = os.path.join(os.getcwd(),'cluster_chains')\n",
    "output_folder = os.path.join(os.getcwd(),'Sentiment_analysis')\n",
    "analyze_and_visualize_clusters(cluster_chains_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308befa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
